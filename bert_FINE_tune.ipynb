{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1557 entries, 0 to 3153\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   lyrics     1557 non-null   object\n",
      " 1   sentiment  1557 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 36.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>I know You've fallen again The way I fell bef...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>You're not alone I've come to take you home Yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>Yeah, like it or not Like a ball and a chain A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>You ought to take us with you when you leave B...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Well, the stairs sound so lonely without you A...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 lyrics  sentiment\n",
       "1593   I know You've fallen again The way I fell bef...          3\n",
       "2493  You're not alone I've come to take you home Yo...          0\n",
       "1572  Yeah, like it or not Like a ball and a chain A...          0\n",
       "2627  You ought to take us with you when you leave B...          3\n",
       "819   Well, the stairs sound so lonely without you A...          2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('lyrics_data_cleaned_grouped.csv', sep=';')\n",
    "df.head()\n",
    "\n",
    "# keep only lyrics and label columns\n",
    "df = df[['lyrics', 'sentiment']]\n",
    "\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.info()\n",
    "\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['sentiment'].unique())}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# replace \\n with space\n",
    "df['lyrics'] = df['lyrics'].str.replace('\\n', ' ')\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['lyrics'], padding='max_length', truncation=True)\n",
    "\n",
    "train_encodings = tokenizer(list(train_df['lyrics']), padding=True, truncation=True)\n",
    "val_encodings = tokenizer(list(val_df['lyrics']), padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LyricsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = LyricsDataset(train_encodings, train_df['sentiment'].tolist())\n",
    "val_dataset = LyricsDataset(val_encodings, val_df['sentiment'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adammillington/anaconda3/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Load accuracy metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edea040f54b494491532473e23d5267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6379, 'grad_norm': 4.786650657653809, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.13}\n",
      "{'loss': 1.6134, 'grad_norm': 6.1113409996032715, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.26}\n",
      "{'loss': 1.5984, 'grad_norm': 7.915622234344482, 'learning_rate': 3e-06, 'epoch': 0.38}\n",
      "{'loss': 1.6087, 'grad_norm': 3.0257174968719482, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.51}\n",
      "{'loss': 1.5566, 'grad_norm': 3.570732593536377, 'learning_rate': 5e-06, 'epoch': 0.64}\n",
      "{'loss': 1.5599, 'grad_norm': 4.370570659637451, 'learning_rate': 6e-06, 'epoch': 0.77}\n",
      "{'loss': 1.5769, 'grad_norm': 4.669127941131592, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cfe50157554addba9c765a565950f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5577938556671143, 'eval_accuracy': 0.2980769230769231, 'eval_runtime': 5.5432, 'eval_samples_per_second': 56.285, 'eval_steps_per_second': 3.608, 'epoch': 1.0}\n",
      "{'loss': 1.5579, 'grad_norm': 3.9423305988311768, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.03}\n",
      "{'loss': 1.5701, 'grad_norm': 4.4329657554626465, 'learning_rate': 9e-06, 'epoch': 1.15}\n",
      "{'loss': 1.5166, 'grad_norm': 7.610838890075684, 'learning_rate': 1e-05, 'epoch': 1.28}\n",
      "{'loss': 1.5468, 'grad_norm': 3.419715642929077, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.41}\n",
      "{'loss': 1.573, 'grad_norm': 3.788599967956543, 'learning_rate': 1.2e-05, 'epoch': 1.54}\n",
      "{'loss': 1.5167, 'grad_norm': 5.615087509155273, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.67}\n",
      "{'loss': 1.5646, 'grad_norm': 3.447225332260132, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.79}\n",
      "{'loss': 1.5458, 'grad_norm': 4.769131183624268, 'learning_rate': 1.5e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc48e391abb84f438a6c616913a7a363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.544020414352417, 'eval_accuracy': 0.2916666666666667, 'eval_runtime': 5.3788, 'eval_samples_per_second': 58.005, 'eval_steps_per_second': 3.718, 'epoch': 2.0}\n",
      "{'loss': 1.5104, 'grad_norm': 7.178765773773193, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.05}\n",
      "{'loss': 1.4983, 'grad_norm': 3.8931076526641846, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.18}\n",
      "{'loss': 1.4822, 'grad_norm': 4.603208065032959, 'learning_rate': 1.8e-05, 'epoch': 2.31}\n",
      "{'loss': 1.5089, 'grad_norm': 4.2160444259643555, 'learning_rate': 1.9e-05, 'epoch': 2.44}\n",
      "{'loss': 1.5926, 'grad_norm': 6.355203151702881, 'learning_rate': 2e-05, 'epoch': 2.56}\n",
      "{'loss': 1.5333, 'grad_norm': 6.801198482513428, 'learning_rate': 2.1e-05, 'epoch': 2.69}\n",
      "{'loss': 1.4435, 'grad_norm': 4.643118858337402, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.82}\n",
      "{'loss': 1.5125, 'grad_norm': 4.888322353363037, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8809bc24c0d4dffb6091187ebaef79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4874439239501953, 'eval_accuracy': 0.34294871794871795, 'eval_runtime': 5.7027, 'eval_samples_per_second': 54.71, 'eval_steps_per_second': 3.507, 'epoch': 3.0}\n",
      "{'train_runtime': 250.1534, 'train_samples_per_second': 14.931, 'train_steps_per_second': 0.935, 'train_loss': 1.5470831129286025, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=234, training_loss=1.5470831129286025, metrics={'train_runtime': 250.1534, 'train_samples_per_second': 14.931, 'train_steps_per_second': 0.935, 'total_flos': 982746262103040.0, 'train_loss': 1.5470831129286025, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0551fb8f7a542369dea973a387ee0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.488988399505615, 'eval_accuracy': 0.03645007923930269, 'eval_runtime': 12.591, 'eval_samples_per_second': 50.115, 'eval_steps_per_second': 3.177, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4f99dd2c814382bc9f076af4b5d13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5801, 'grad_norm': 8.692593574523926, 'learning_rate': 1.978902953586498e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6026, 'grad_norm': 6.65548849105835, 'learning_rate': 1.957805907172996e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5784, 'grad_norm': 8.007187843322754, 'learning_rate': 1.936708860759494e-05, 'epoch': 0.09}\n",
      "{'loss': 4.6495, 'grad_norm': 8.252561569213867, 'learning_rate': 1.9156118143459917e-05, 'epoch': 0.13}\n",
      "{'loss': 4.6157, 'grad_norm': 7.027004718780518, 'learning_rate': 1.8945147679324897e-05, 'epoch': 0.16}\n",
      "{'loss': 4.6453, 'grad_norm': 6.201651573181152, 'learning_rate': 1.8734177215189874e-05, 'epoch': 0.19}\n",
      "{'loss': 4.5781, 'grad_norm': 5.533116340637207, 'learning_rate': 1.8523206751054853e-05, 'epoch': 0.22}\n",
      "{'loss': 4.6104, 'grad_norm': 5.384045124053955, 'learning_rate': 1.8312236286919833e-05, 'epoch': 0.25}\n",
      "{'loss': 4.5855, 'grad_norm': 5.016408443450928, 'learning_rate': 1.8101265822784813e-05, 'epoch': 0.28}\n",
      "{'loss': 4.5836, 'grad_norm': 5.545433044433594, 'learning_rate': 1.789029535864979e-05, 'epoch': 0.32}\n",
      "{'loss': 4.59, 'grad_norm': 5.961114406585693, 'learning_rate': 1.767932489451477e-05, 'epoch': 0.35}\n",
      "{'loss': 4.5631, 'grad_norm': 6.388518810272217, 'learning_rate': 1.746835443037975e-05, 'epoch': 0.38}\n",
      "{'loss': 4.5885, 'grad_norm': 5.709355354309082, 'learning_rate': 1.725738396624473e-05, 'epoch': 0.41}\n",
      "{'loss': 4.5841, 'grad_norm': 6.291782855987549, 'learning_rate': 1.7046413502109705e-05, 'epoch': 0.44}\n",
      "{'loss': 4.5415, 'grad_norm': 5.337477207183838, 'learning_rate': 1.6835443037974685e-05, 'epoch': 0.47}\n",
      "{'loss': 4.5684, 'grad_norm': 6.160799980163574, 'learning_rate': 1.662447257383966e-05, 'epoch': 0.51}\n",
      "{'loss': 4.5988, 'grad_norm': 5.758630752563477, 'learning_rate': 1.641350210970464e-05, 'epoch': 0.54}\n",
      "{'loss': 4.5854, 'grad_norm': 5.096074104309082, 'learning_rate': 1.620253164556962e-05, 'epoch': 0.57}\n",
      "{'loss': 4.5761, 'grad_norm': 5.176407337188721, 'learning_rate': 1.59915611814346e-05, 'epoch': 0.6}\n",
      "{'loss': 4.5758, 'grad_norm': 5.728793144226074, 'learning_rate': 1.578059071729958e-05, 'epoch': 0.63}\n",
      "{'loss': 4.5836, 'grad_norm': 4.949633598327637, 'learning_rate': 1.556962025316456e-05, 'epoch': 0.66}\n",
      "{'loss': 4.5918, 'grad_norm': 4.552150726318359, 'learning_rate': 1.5358649789029537e-05, 'epoch': 0.7}\n",
      "{'loss': 4.5906, 'grad_norm': 4.381960391998291, 'learning_rate': 1.5147679324894515e-05, 'epoch': 0.73}\n",
      "{'loss': 4.5762, 'grad_norm': 4.270885467529297, 'learning_rate': 1.4936708860759495e-05, 'epoch': 0.76}\n",
      "{'loss': 4.5789, 'grad_norm': 3.9369959831237793, 'learning_rate': 1.4725738396624474e-05, 'epoch': 0.79}\n",
      "{'loss': 4.5601, 'grad_norm': 4.41865873336792, 'learning_rate': 1.4514767932489453e-05, 'epoch': 0.82}\n",
      "{'loss': 4.5705, 'grad_norm': 4.941321849822998, 'learning_rate': 1.4303797468354432e-05, 'epoch': 0.85}\n",
      "{'loss': 4.5661, 'grad_norm': 5.256267070770264, 'learning_rate': 1.4092827004219412e-05, 'epoch': 0.89}\n",
      "{'loss': 4.5869, 'grad_norm': 5.660477638244629, 'learning_rate': 1.3881856540084389e-05, 'epoch': 0.92}\n",
      "{'loss': 4.5746, 'grad_norm': 5.542744159698486, 'learning_rate': 1.3670886075949368e-05, 'epoch': 0.95}\n",
      "{'loss': 4.5056, 'grad_norm': 5.668625354766846, 'learning_rate': 1.3459915611814346e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdda3de9f60e40f08ea18c89fb8ee8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.551388740539551, 'eval_accuracy': 0.01901743264659271, 'eval_runtime': 11.4352, 'eval_samples_per_second': 55.181, 'eval_steps_per_second': 6.909, 'epoch': 1.0}\n",
      "{'loss': 4.5396, 'grad_norm': 5.505777835845947, 'learning_rate': 1.3248945147679326e-05, 'epoch': 1.01}\n",
      "{'loss': 4.5185, 'grad_norm': 5.721272945404053, 'learning_rate': 1.3037974683544306e-05, 'epoch': 1.04}\n",
      "{'loss': 4.4947, 'grad_norm': 5.639856338500977, 'learning_rate': 1.2827004219409284e-05, 'epoch': 1.08}\n",
      "{'loss': 4.4905, 'grad_norm': 5.564944267272949, 'learning_rate': 1.2616033755274262e-05, 'epoch': 1.11}\n",
      "{'loss': 4.5117, 'grad_norm': 6.717665672302246, 'learning_rate': 1.240506329113924e-05, 'epoch': 1.14}\n",
      "{'loss': 4.5348, 'grad_norm': 5.911284923553467, 'learning_rate': 1.219409282700422e-05, 'epoch': 1.17}\n",
      "{'loss': 4.5365, 'grad_norm': 6.411997318267822, 'learning_rate': 1.19831223628692e-05, 'epoch': 1.2}\n",
      "{'loss': 4.5199, 'grad_norm': 6.429394721984863, 'learning_rate': 1.1772151898734178e-05, 'epoch': 1.23}\n",
      "{'loss': 4.5064, 'grad_norm': 5.570857048034668, 'learning_rate': 1.1561181434599158e-05, 'epoch': 1.27}\n",
      "{'loss': 4.478, 'grad_norm': 6.009703636169434, 'learning_rate': 1.1350210970464134e-05, 'epoch': 1.3}\n",
      "{'loss': 4.5661, 'grad_norm': 6.287379741668701, 'learning_rate': 1.1139240506329114e-05, 'epoch': 1.33}\n",
      "{'loss': 4.4933, 'grad_norm': 5.870284557342529, 'learning_rate': 1.0928270042194094e-05, 'epoch': 1.36}\n",
      "{'loss': 4.5322, 'grad_norm': 5.770760536193848, 'learning_rate': 1.0717299578059072e-05, 'epoch': 1.39}\n",
      "{'loss': 4.5229, 'grad_norm': 5.912240505218506, 'learning_rate': 1.0506329113924052e-05, 'epoch': 1.42}\n",
      "{'loss': 4.4999, 'grad_norm': 5.790663242340088, 'learning_rate': 1.0295358649789031e-05, 'epoch': 1.46}\n",
      "{'loss': 4.5176, 'grad_norm': 5.718502521514893, 'learning_rate': 1.0084388185654008e-05, 'epoch': 1.49}\n",
      "{'loss': 4.6046, 'grad_norm': 5.703424453735352, 'learning_rate': 9.87341772151899e-06, 'epoch': 1.52}\n",
      "{'loss': 4.5198, 'grad_norm': 6.126877307891846, 'learning_rate': 9.662447257383967e-06, 'epoch': 1.55}\n",
      "{'loss': 4.5637, 'grad_norm': 5.526630401611328, 'learning_rate': 9.451476793248946e-06, 'epoch': 1.58}\n",
      "{'loss': 4.5475, 'grad_norm': 5.164228439331055, 'learning_rate': 9.240506329113925e-06, 'epoch': 1.61}\n",
      "{'loss': 4.4949, 'grad_norm': 7.2146525382995605, 'learning_rate': 9.029535864978903e-06, 'epoch': 1.65}\n",
      "{'loss': 4.4501, 'grad_norm': 6.267153263092041, 'learning_rate': 8.818565400843883e-06, 'epoch': 1.68}\n",
      "{'loss': 4.5407, 'grad_norm': 6.3176703453063965, 'learning_rate': 8.607594936708861e-06, 'epoch': 1.71}\n",
      "{'loss': 4.5532, 'grad_norm': 6.072173595428467, 'learning_rate': 8.39662447257384e-06, 'epoch': 1.74}\n",
      "{'loss': 4.5069, 'grad_norm': 6.295547008514404, 'learning_rate': 8.18565400843882e-06, 'epoch': 1.77}\n",
      "{'loss': 4.5642, 'grad_norm': 5.690882205963135, 'learning_rate': 7.974683544303799e-06, 'epoch': 1.8}\n",
      "{'loss': 4.5051, 'grad_norm': 6.1843180656433105, 'learning_rate': 7.763713080168777e-06, 'epoch': 1.84}\n",
      "{'loss': 4.5657, 'grad_norm': 5.896661281585693, 'learning_rate': 7.552742616033756e-06, 'epoch': 1.87}\n",
      "{'loss': 4.5886, 'grad_norm': 6.542744159698486, 'learning_rate': 7.341772151898735e-06, 'epoch': 1.9}\n",
      "{'loss': 4.5457, 'grad_norm': 5.4123101234436035, 'learning_rate': 7.130801687763713e-06, 'epoch': 1.93}\n",
      "{'loss': 4.4849, 'grad_norm': 5.7256669998168945, 'learning_rate': 6.919831223628692e-06, 'epoch': 1.96}\n",
      "{'loss': 4.5046, 'grad_norm': 5.883993625640869, 'learning_rate': 6.708860759493672e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ea689243c34c2a85ae6104b096f170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.5236358642578125, 'eval_accuracy': 0.022187004754358162, 'eval_runtime': 15.6275, 'eval_samples_per_second': 40.377, 'eval_steps_per_second': 5.055, 'epoch': 2.0}\n",
      "{'loss': 4.486, 'grad_norm': 5.4746599197387695, 'learning_rate': 6.49789029535865e-06, 'epoch': 2.03}\n",
      "{'loss': 4.5171, 'grad_norm': 6.879508018493652, 'learning_rate': 6.286919831223629e-06, 'epoch': 2.06}\n",
      "{'loss': 4.4685, 'grad_norm': 6.099058628082275, 'learning_rate': 6.075949367088608e-06, 'epoch': 2.09}\n",
      "{'loss': 4.4667, 'grad_norm': 6.156914234161377, 'learning_rate': 5.864978902953588e-06, 'epoch': 2.12}\n",
      "{'loss': 4.419, 'grad_norm': 6.9019246101379395, 'learning_rate': 5.654008438818566e-06, 'epoch': 2.15}\n",
      "{'loss': 4.4801, 'grad_norm': 8.061492919921875, 'learning_rate': 5.443037974683545e-06, 'epoch': 2.18}\n",
      "{'loss': 4.499, 'grad_norm': 6.680893898010254, 'learning_rate': 5.2320675105485245e-06, 'epoch': 2.22}\n",
      "{'loss': 4.421, 'grad_norm': 7.623788833618164, 'learning_rate': 5.021097046413503e-06, 'epoch': 2.25}\n",
      "{'loss': 4.4036, 'grad_norm': 6.920340061187744, 'learning_rate': 4.8101265822784815e-06, 'epoch': 2.28}\n",
      "{'loss': 4.3929, 'grad_norm': 7.818154811859131, 'learning_rate': 4.5991561181434605e-06, 'epoch': 2.31}\n",
      "{'loss': 4.4222, 'grad_norm': 7.119595050811768, 'learning_rate': 4.3881856540084394e-06, 'epoch': 2.34}\n",
      "{'loss': 4.427, 'grad_norm': 6.913540363311768, 'learning_rate': 4.177215189873418e-06, 'epoch': 2.37}\n",
      "{'loss': 4.4328, 'grad_norm': 7.734883785247803, 'learning_rate': 3.9662447257383965e-06, 'epoch': 2.41}\n",
      "{'loss': 4.4352, 'grad_norm': 7.2451276779174805, 'learning_rate': 3.755274261603376e-06, 'epoch': 2.44}\n",
      "{'loss': 4.379, 'grad_norm': 7.821078777313232, 'learning_rate': 3.544303797468355e-06, 'epoch': 2.47}\n",
      "{'loss': 4.363, 'grad_norm': 8.237737655639648, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
      "{'loss': 4.4007, 'grad_norm': 8.169037818908691, 'learning_rate': 3.1223628691983127e-06, 'epoch': 2.53}\n",
      "{'loss': 4.3102, 'grad_norm': 7.042150497436523, 'learning_rate': 2.9113924050632912e-06, 'epoch': 2.56}\n",
      "{'loss': 4.4145, 'grad_norm': 8.211234092712402, 'learning_rate': 2.70042194092827e-06, 'epoch': 2.59}\n",
      "{'loss': 4.4086, 'grad_norm': 9.091883659362793, 'learning_rate': 2.489451476793249e-06, 'epoch': 2.63}\n",
      "{'loss': 4.4656, 'grad_norm': 7.615543842315674, 'learning_rate': 2.278481012658228e-06, 'epoch': 2.66}\n",
      "{'loss': 4.4378, 'grad_norm': 7.693934917449951, 'learning_rate': 2.067510548523207e-06, 'epoch': 2.69}\n",
      "{'loss': 4.4051, 'grad_norm': 8.15017318725586, 'learning_rate': 1.856540084388186e-06, 'epoch': 2.72}\n",
      "{'loss': 4.4524, 'grad_norm': 7.59888219833374, 'learning_rate': 1.6455696202531647e-06, 'epoch': 2.75}\n",
      "{'loss': 4.3963, 'grad_norm': 8.114371299743652, 'learning_rate': 1.4345991561181436e-06, 'epoch': 2.78}\n",
      "{'loss': 4.5253, 'grad_norm': 7.460914611816406, 'learning_rate': 1.2236286919831226e-06, 'epoch': 2.82}\n",
      "{'loss': 4.4567, 'grad_norm': 7.548434257507324, 'learning_rate': 1.0126582278481013e-06, 'epoch': 2.85}\n",
      "{'loss': 4.4468, 'grad_norm': 7.209286689758301, 'learning_rate': 8.016877637130803e-07, 'epoch': 2.88}\n",
      "{'loss': 4.3908, 'grad_norm': 9.02324104309082, 'learning_rate': 5.907172995780591e-07, 'epoch': 2.91}\n",
      "{'loss': 4.3837, 'grad_norm': 7.891607761383057, 'learning_rate': 3.79746835443038e-07, 'epoch': 2.94}\n",
      "{'loss': 4.3733, 'grad_norm': 7.5267438888549805, 'learning_rate': 1.6877637130801689e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9cfaff653e4dd4b37c790e365568d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.509838581085205, 'eval_accuracy': 0.02377179080824089, 'eval_runtime': 15.5205, 'eval_samples_per_second': 40.656, 'eval_steps_per_second': 5.09, 'epoch': 3.0}\n",
      "{'train_runtime': 667.1738, 'train_samples_per_second': 11.345, 'train_steps_per_second': 1.421, 'train_loss': 4.512006864266054, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6b7eec9e064acab6ecc654d99e13f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac8667bce2e4684bc5cc0b97dc0168d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6903, 'grad_norm': 8.00285816192627, 'learning_rate': 1.978902953586498e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6399, 'grad_norm': 6.19515323638916, 'learning_rate': 1.957805907172996e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5861, 'grad_norm': 6.1281962394714355, 'learning_rate': 1.936708860759494e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 41\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m)\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     43\u001b[0m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m params\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [2e-5, 3e-5, 5e-5],\n",
    "    'per_device_train_batch_size': [8, 16, 32],\n",
    "    'num_train_epochs': [3, 4, 5],\n",
    "    'warmup_steps': [0, 500],\n",
    "    'weight_decay': [0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for params in ParameterGrid(param_grid, ):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=params['learning_rate'],\n",
    "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=params['per_device_train_batch_size'],\n",
    "        num_train_epochs=params['num_train_epochs'],\n",
    "        warmup_steps=params['warmup_steps'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=99)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_results['params'] = params\n",
    "    results.append(eval_results)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_result = min(results, key=lambda x: x['eval_loss'])\n",
    "print(\"Best hyperparameters:\", best_result['params'])\n",
    "print(\"Best evaluation loss:\", best_result['eval_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_loss': 4.509838581085205,\n",
       "  'eval_accuracy': 0.02377179080824089,\n",
       "  'eval_runtime': 15.2409,\n",
       "  'eval_samples_per_second': 41.402,\n",
       "  'eval_steps_per_second': 5.183,\n",
       "  'epoch': 3.0,\n",
       "  'params': {'learning_rate': 2e-05,\n",
       "   'num_train_epochs': 3,\n",
       "   'per_device_train_batch_size': 8,\n",
       "   'warmup_steps': 0,\n",
       "   'weight_decay': 0.01}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
